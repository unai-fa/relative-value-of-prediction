{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b8eab3d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LassoCV"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97c540be",
   "metadata": {},
   "source": [
    "# Data Preprocessing and Model Training\n",
    "\n",
    "This notebook processes Ethiopia LSMS (Living Standards Measurement Survey) 2015 data to create a consumption prediction model for poverty targeting.\n",
    "\n",
    "**Pipeline:**\n",
    "1. Load and clean survey data (consumption, assets, housing, ...)\n",
    "2. Merge into a single household-level dataset\n",
    "3. Train a LASSO model to predict consumption from observable characteristics\n",
    "\n",
    "**Data source:** The survey data can be downloaded from the LSMS program [here](https://microdata.worldbank.org/index.php/catalog/2783).\n",
    "\n",
    "**Output:** `survey2015.csv` â€” processed dataset with predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c650e94",
   "metadata": {},
   "outputs": [],
   "source": [
    "root = 'PATH-TO_ROOT-FOLDER'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93bb7c03",
   "metadata": {},
   "source": [
    "## Preprocessing "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9d9bbac",
   "metadata": {},
   "source": [
    "### Consumption data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e522a1bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Households with consumption data: 4717\n"
     ]
    }
   ],
   "source": [
    "# Read consumption data, get HHID + consumption HH weight columns\n",
    "cons = pd.read_stata(root + 'cons_agg_w3.dta')\n",
    "cons = cons[['household_id', 'nom_totcons_aeq', 'adulteq']]\n",
    "cons.columns = ['hhid', 'Per Capita Consumption', 'Adults']\n",
    "            \n",
    "# Construct total consumption from per capita consumption\n",
    "cons['Consumption'] = cons['Per Capita Consumption']*cons['Adults']\n",
    "cons = cons[['hhid', 'Consumption']]\n",
    "\n",
    "# Drop any households with missing ID or missing consumption\n",
    "cons['hhid'] = cons['hhid'].apply(lambda x: np.nan if x == '' else x) \n",
    "cons = cons.dropna(how='any')\n",
    "\n",
    "# Convert to Birr per year to USD PPP per year\n",
    "exchange_rate = 20.13\n",
    "cons['Consumption'] = cons['Consumption']/exchange_rate\n",
    "print('Households with consumption data: %i' % len(cons))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e796d438",
   "metadata": {},
   "source": [
    "### Asset data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b8b915c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Households with asset data: 4951\n"
     ]
    }
   ],
   "source": [
    "# Get asset data from asset module \n",
    "assets = pd.read_stata(root + 'Household/sect10_hh_w3.dta')\n",
    "assets = assets[['household_id', 'hh_s10q00', 'hh_s10q01']]\n",
    "assets['household_id'] = assets['household_id'].apply(lambda x: np.nan if x == '' else x)\n",
    "\n",
    "# Drop households missing ID and duplicate records\n",
    "assets = assets.dropna(subset=['household_id'])\n",
    "assets = assets.drop_duplicates(subset=['household_id', 'hh_s10q00'])\n",
    "\n",
    "assets[assets['household_id'].isin(['13010100303004', '01050100105031', '13010100303032'])]\n",
    "\n",
    "# Pivot to per-household dataset \n",
    "assets = assets.pivot(columns=['hh_s10q00'], index=['household_id'])\n",
    "assets.columns = [str(c[1]) for c in assets.columns]\n",
    "assets['hhid'] = assets.index\n",
    "assets.index = range(len(assets))\n",
    "assets = assets.fillna(0)\n",
    "assets.columns = ['Kerosene stove', 'Gas stove', 'Electric stove', 'Blanket', 'Bed', 'Clock', 'Telephone',\n",
    "                 'Mobile phone', 'Radio', 'TV', 'CD/DVD', 'Satellite dish', 'Sofa', 'Bicycle', 'Motorcycle',\n",
    "                 'Hand cart', 'Animal-drawn cart', 'Sewing machine', 'Weaving equipment', 'Mitad', \n",
    "                  'Energy-saving stove', 'Refridgerator', 'Car', 'Jewels', 'Wardrobe', 'Shelf', 'Biogas stove',\n",
    "                 'Water storage pit', 'Mofer and kember', 'Sickle', 'Axe', 'Pick axe', 'Plough (traditional)',\n",
    "                 'Plough (modern)', 'Water pump', 'hhid']\n",
    "print('Households with asset data: %i' % len(assets))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "501d79b5",
   "metadata": {},
   "source": [
    "### Housing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dcd27ba9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Households with housing data: 4954\n"
     ]
    }
   ],
   "source": [
    "# Get housing data from housing module\n",
    "housing = pd.read_stata(root + 'Household/sect9_hh_w3.dta', convert_categoricals=False)\n",
    "housing = housing[['household_id', 'hh_s9q03', 'hh_s9q04', 'hh_s9q05', 'hh_s9q06', 'hh_s9q07', 'hh_s9q08', \n",
    "                   'hh_s9q09',  'hh_s9q12', 'hh_s9q17', 'hh_s9q19_a', 'hh_s9q21']]\n",
    "colnames = ['hhid', 'Ownership status', 'Rooms', 'Walls', 'Roof', 'Floor', 'Kitchen', 'Oven', 'Garbage', \n",
    "            'Other houses', 'Lighting fuel', 'Cooking fuel']\n",
    "housing.columns = colnames\n",
    "\n",
    "# Drop households missing IDs, no other column has missingness\n",
    "housing['hhid'] = housing['hhid'].apply(lambda x: np.nan if x == '' else x)\n",
    "housing = housing.dropna()\n",
    "\n",
    "print('Households with housing data: %i' % len(housing))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7794e212",
   "metadata": {},
   "source": [
    "### Demographic data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0f9db8e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Households with demographic data: 4953\n"
     ]
    }
   ],
   "source": [
    "# Get demographic data from individual rsoter module \n",
    "demographics = pd.read_stata(root + 'Household/sect1_hh_w3.dta', convert_categoricals=False)\n",
    "demographics = demographics[['household_id', 'hh_s1q02', 'hh_s1q03', 'hh_s1q04a', 'hh_s1q04b', 'hh_s1q08']]\n",
    "demographics.columns = ['hhid', 'hhh', 'gender', 'age1', 'age2', 'married']\n",
    "\n",
    "# Get age -- use age of household head if available, infer household head age as second member's age if not\n",
    "demographics['age'] = demographics.apply(lambda row: row['age1'] if not pd.isnull(row['age1'])\n",
    "                                        else row['age2'] if not pd.isnull(row['age2'])\n",
    "                                        else np.nan, axis=1)\n",
    "\n",
    "# Add missing category to martical status\n",
    "demographics['married'] = demographics['married'].fillna(0)\n",
    "\n",
    "# Drop any rows missing ID\n",
    "demographics['hhid'] = demographics['hhid'].apply(lambda x: np.nan if x == '' else x)\n",
    "demographics = demographics.dropna(subset=['hhid'])\n",
    "\n",
    "# Create dataframe of household head data\n",
    "hhh = demographics[demographics['hhh'] == 1].copy()\n",
    "hhh = hhh[['hhid', 'gender', 'age', 'married']]\n",
    "hhh.columns = ['hhid', 'HHH gender', 'HHH age', 'HHH married']\n",
    "\n",
    "# Create dataframe with number of children in household\n",
    "children = demographics[demographics['age'] < 5].copy()\n",
    "children = children[['hhid', 'gender']].groupby('hhid', as_index=False).agg('count')\\\n",
    "    .rename({'gender':'Children'}, axis=1)\n",
    "\n",
    "# Create dataframe with other misc demographic data -- region, number of members\n",
    "geo = pd.read_stata(root + 'cons_agg_w3.dta', convert_categoricals=False)\n",
    "geo = geo[['household_id', 'saq01', 'hh_size']]\n",
    "geo.columns = ['hhid', 'region', 'hh_size']\n",
    "\n",
    "# Merge all demographic data together\n",
    "demographics = geo.merge(hhh, on='hhid', how='left').merge(children, on='hhid', how='left')\n",
    "demographics['Children'] = demographics['Children'].fillna(0)\n",
    "demographics = demographics.dropna()\n",
    "print('Households with demographic data: %i' % len(demographics))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54861d2b",
   "metadata": {},
   "source": [
    "### Education data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bf7a7fbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Households with education data: 4953\n"
     ]
    }
   ],
   "source": [
    "# Get IDs for just household heads\n",
    "hhh = pd.read_stata(root + '/Household/sect1_hh_w3.dta', convert_categoricals=False)\n",
    "hhh = hhh[['household_id', 'individual_id2', 'hh_s1q02']]\n",
    "hhh.columns = ['hhid', 'iid', 'hhh']\n",
    "hhh = hhh[hhh['hhh'] == 1][['hhid', 'iid']]\n",
    "\n",
    "# Read education data from individual roster module \n",
    "education = pd.read_stata(root + '/Household/sect2_hh_w3.dta', convert_categoricals=False)\n",
    "education = education[['individual_id2', 'hh_s2q02', 'hh_s2q03', 'hh_s2q05']]\n",
    "education.columns = ['iid', 'literate', 'school', 'level_school']\n",
    "\n",
    "# Merge education data to IDs to get education data for just household heads\n",
    "education = education.merge(hhh, on='iid', how='inner').drop('iid', axis=1)\n",
    "\n",
    "# Recode literacy and education variables \n",
    "education['literate'] = 1 - (education['literate'] - 1)\n",
    "education['level_school'] = education.apply(lambda row: -1 if row['school'] == 2 else \n",
    "                                            row['level_school'], axis=1)\n",
    "education = education.drop('school', axis=1)\n",
    "education['level_school'] = education['level_school'].fillna(-1)\n",
    "\n",
    "print('Households with education data: %i' % len(demographics))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bcefbdc",
   "metadata": {},
   "source": [
    "### Merge all data together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6e5a1f5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Households in final dataset: 4694\n"
     ]
    }
   ],
   "source": [
    "# Combine together all dataframes (HH need to have consumption data, can have other data missing)\n",
    "df = cons.merge(demographics, on='hhid', how='left').merge(housing, on='hhid', how='left')\\\n",
    "    .merge(assets, on='hhid', how='left').merge(education, on='hhid', how='left')\\\n",
    "    .dropna()\\\n",
    "    .drop_duplicates(subset=['hhid'])\n",
    "\n",
    "# Get outcome and feature variables\n",
    "outcome = 'Consumption'\n",
    "x_vars = [c for c in df.columns if c not in ['hhid', outcome]]\n",
    "\n",
    "# Define type for each feature variable manually \n",
    "x_var_types = ['cat', 'int', 'bin', 'int', 'cat', 'int', 'cat', 'int', 'cat', 'cat', 'cat', 'cat',\n",
    "              'cat', 'cat', 'bin', 'cat', 'cat'] + ['int']*(len(assets.columns) - 1) + ['bin', 'cat']\n",
    "\n",
    "for v, var in enumerate(x_vars):\n",
    "    \n",
    "    # Check binary variables for having exactly 2 values, and convert values to 0 and 1\n",
    "    if x_var_types[v] == 'bin':\n",
    "        if len(df[var].unique()) != 2:\n",
    "            print('Error: Unique values in column ' + var + ' not equal to 2')\n",
    "        if 2 in df[var].unique():\n",
    "            df[var] = df[var] - 1\n",
    "            \n",
    "    # Winsorize then scale continuous variables, and drop any that have 100% missingness or are constant\n",
    "    elif x_var_types[v] == 'int':\n",
    "        cutoff = np.percentile(df[var], 99)\n",
    "        df[var] = df[var].apply(lambda x: cutoff if x > cutoff else x)\n",
    "        df[var] = (df[var] - df[var].min())/(df[var].max() - df[var].min())\n",
    "        if df[var].isnull().mean() == 1:\n",
    "            df = df.drop(var, axis=1)\n",
    "\n",
    "# Combine any categories in categorical variables with <1% of observations into an \"other\" category\n",
    "# and one hot encode\n",
    "cat_vars = [x_vars[v] for v in range(len(x_vars)) if x_var_types[v] == 'cat']\n",
    "for cat_var in cat_vars:\n",
    "    to_remove = []\n",
    "    grouped = pd.DataFrame(df.groupby(cat_var).count()['hhid']/len(df))\n",
    "    for value, share in grouped.iterrows():\n",
    "        if share[0] < 0.01:\n",
    "            to_remove.append(value)\n",
    "    df[cat_var] = df[cat_var].apply(lambda x: 9999 if x in to_remove else x)\n",
    "df = pd.get_dummies(df, columns=cat_vars, drop_first=True)\n",
    "\n",
    "df.to_csv('survey2015.csv', index=False)\n",
    "print('Households in final dataset: %i' % len(df))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e84bd69",
   "metadata": {},
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a005b2bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_consumption_model(random_state=0):\n",
    "    \"\"\"Train LASSO model to predict consumption from survey features.\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame with columns 'y_test' (actual) and 'yhat_test' (predicted),\n",
    "        both in USD PPP.\n",
    "    \"\"\"\n",
    "    df = pd.read_csv('survey2015.csv')\n",
    "    \n",
    "    outcome = 'Consumption'\n",
    "    extras = ['hhid']\n",
    "    \n",
    "    # Train/test split, log-transform consumption\n",
    "    train, test = train_test_split(df, random_state=random_state, test_size=0.25)\n",
    "    x_train = train.drop([outcome] + extras, axis=1)\n",
    "    y_train = np.log(train[outcome])\n",
    "    x_test = test.drop([outcome] + extras, axis=1)\n",
    "    y_test = np.log(test[outcome])\n",
    "    \n",
    "    # Train LASSO with cross-validated alpha\n",
    "    model = LassoCV(fit_intercept=True, random_state=random_state)\n",
    "    model.fit(x_train, y_train)\n",
    "    yhat_test = model.predict(x_test)\n",
    "    \n",
    "    # Back-transform to USD PPP\n",
    "    results = pd.DataFrame({\n",
    "        'y_test': np.exp(y_test),\n",
    "        'yhat_test': np.exp(yhat_test)\n",
    "    })\n",
    "    \n",
    "    return results\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data-science",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
